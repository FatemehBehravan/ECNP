Saved Details
ANPModel(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=5, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention()
  )
  (_decoder): ANPDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=130, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=6, bias=True)
    )
  )
)
Num parameters: 100870
Namespace(dataset='XAUUSD', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='CNP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=3)
Saved Details
ANPModel(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=5, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention()
  )
  (_decoder): ANPDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=130, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=6, bias=True)
    )
  )
)
Num parameters: 100870
Namespace(dataset='XAUUSD', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='CNP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=3)
Saved Details
ANPModel(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=5, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention()
  )
  (_decoder): ANPDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=130, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=6, bias=True)
    )
  )
)
Num parameters: 100870
Namespace(dataset='XAUUSD', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='CNP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=3)
Saved Details
ANPModel(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=5, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention()
  )
  (_decoder): ANPDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=130, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=6, bias=True)
    )
  )
)
Num parameters: 100870
Namespace(dataset='XAUUSD', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='CNP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=3)
Saved Details
ANPModel(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=5, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention()
  )
  (_decoder): ANPDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=130, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=6, bias=True)
    )
  )
)
Num parameters: 100870
Namespace(dataset='XAUUSD', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='CNP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=3)
Saved Details
ANPModel(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=5, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention()
  )
  (_decoder): ANPDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=130, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=6, bias=True)
    )
  )
)
Num parameters: 100870
Namespace(dataset='XAUUSD', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='CNP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=3)
Saved Details
ANPModel(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=5, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention()
  )
  (_decoder): ANPDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=130, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=6, bias=True)
    )
  )
)
Num parameters: 100870
Namespace(dataset='XAUUSD', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='CNP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=3)
Saved Details
ANPModel(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=5, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention()
  )
  (_decoder): ANPDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=130, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=6, bias=True)
    )
  )
)
Num parameters: 100870
Namespace(dataset='XAUUSD', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='CNP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=3)
Saved Details
ANPModel(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=5, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention()
  )
  (_decoder): ANPDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=130, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=6, bias=True)
    )
  )
)
Num parameters: 100870
Namespace(dataset='XAUUSD', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='CNP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=3)
Saved Details
ANPModel(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=5, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention(
      (linear_layers_list): ModuleList(
        (0): Linear(in_features=2, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (linear_layers_list2): ModuleList(
        (0): Linear(in_features=2, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (multihead_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
      )
    )
  )
  (_decoder): ANPDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=130, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=6, bias=True)
    )
  )
)
Num parameters: 200710
Namespace(dataset='XAUUSD', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='ANP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=3)
Saved Details
ANPModel(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=5, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention(
      (linear_layers_list): ModuleList(
        (0): Linear(in_features=2, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (linear_layers_list2): ModuleList(
        (0): Linear(in_features=2, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (multihead_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
      )
    )
  )
  (_decoder): ANPDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=130, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=6, bias=True)
    )
  )
)
Num parameters: 200710
Namespace(dataset='XAUUSD', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='ANP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=3)
Saved Details
ANPModel(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=5, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention(
      (linear_layers_list): ModuleList(
        (0): Linear(in_features=2, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (linear_layers_list2): ModuleList(
        (0): Linear(in_features=2, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (multihead_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
      )
    )
  )
  (_decoder): ANPDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=130, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=6, bias=True)
    )
  )
)
Num parameters: 200710
Namespace(dataset='XAUUSD', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='ANP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=3)
Saved Details
ANPModel(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=5, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention(
      (linear_layers_list): ModuleList(
        (0): Linear(in_features=2, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (linear_layers_list2): ModuleList(
        (0): Linear(in_features=2, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (multihead_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
      )
    )
  )
  (_decoder): ANPDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=130, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=6, bias=True)
    )
  )
)
Num parameters: 200710
Namespace(dataset='XAUUSD', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='ANP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=3)
Saved Details
ANPModel(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=5, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention(
      (linear_layers_list): ModuleList(
        (0): Linear(in_features=2, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (linear_layers_list2): ModuleList(
        (0): Linear(in_features=2, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (multihead_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
      )
    )
  )
  (_decoder): ANPDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=130, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=6, bias=True)
    )
  )
)
Num parameters: 200710
Namespace(dataset='XAUUSD', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='ANP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=3)
Saved Details
ANPModel(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=5, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention(
      (linear_layers_list): ModuleList(
        (0): Linear(in_features=2, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (linear_layers_list2): ModuleList(
        (0): Linear(in_features=2, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (multihead_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
      )
    )
  )
  (_decoder): ANPDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=130, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=6, bias=True)
    )
  )
)
Num parameters: 200710
Namespace(dataset='XAUUSD', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='ANP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=3)
Saved Details
ANPModel(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=5, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention(
      (linear_layers_list): ModuleList(
        (0): Linear(in_features=2, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (linear_layers_list2): ModuleList(
        (0): Linear(in_features=2, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (multihead_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
      )
    )
  )
  (_decoder): ANPDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=130, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=6, bias=True)
    )
  )
)
Num parameters: 200710
Namespace(dataset='XAUUSD', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='ANP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=3)
Saved Details
ANPModel(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=5, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention(
      (linear_layers_list): ModuleList(
        (0): Linear(in_features=2, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (linear_layers_list2): ModuleList(
        (0): Linear(in_features=2, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (multihead_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
      )
    )
  )
  (_decoder): ANPDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=130, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=6, bias=True)
    )
  )
)
Num parameters: 200710
Namespace(dataset='XAUUSD', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='ANP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=3)
Saved Details
ANPModel(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=2, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention(
      (linear_layers_list): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (linear_layers_list2): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (multihead_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
      )
    )
  )
  (_decoder): ANPDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=129, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
Num parameters: 199426
Namespace(dataset='XAUUSD', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='ANP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=1)
Saved Details
ANPModel(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=2, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention(
      (linear_layers_list): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (linear_layers_list2): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (multihead_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
      )
    )
  )
  (_decoder): ANPDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=129, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
Num parameters: 199426
Namespace(dataset='XAUUSD', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='ANP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=1)
Saved Details
ANPModel(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=2, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention(
      (linear_layers_list): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (linear_layers_list2): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (multihead_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
      )
    )
  )
  (_decoder): ANPDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=129, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
Num parameters: 199426
Namespace(dataset='XAUUSD', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='ANP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=1)
Saved Details
ANPModel(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=2, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention(
      (linear_layers_list): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (linear_layers_list2): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (multihead_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
      )
    )
  )
  (_decoder): ANPDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=129, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
Num parameters: 199426
Namespace(dataset='XAUUSD', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='ANP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=1)
Saved Details
ANPModel(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=2, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention(
      (linear_layers_list): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (linear_layers_list2): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (multihead_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
      )
    )
  )
  (_decoder): ANPDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=129, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
Num parameters: 199426
Namespace(dataset='XAUUSD', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='ANP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=1)
Saved Details
ANPModel(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=2, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention(
      (linear_layers_list): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (linear_layers_list2): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (multihead_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
      )
    )
  )
  (_decoder): ANPDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=129, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
Num parameters: 199426
Namespace(dataset='XAUUSD', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='ANP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=1)
Saved Details
ANPModel(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=2, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention(
      (linear_layers_list): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (linear_layers_list2): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (multihead_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
      )
    )
  )
  (_decoder): ANPDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=129, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
Num parameters: 199426
Namespace(dataset='XAUUSD', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='ANP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=1)
Saved Details
ANPModel(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=2, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention(
      (linear_layers_list): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (linear_layers_list2): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (multihead_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
      )
    )
  )
  (_decoder): ANPDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=129, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
Num parameters: 199426
Namespace(dataset='XAUUSD', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='ANP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=1)
Saved Details
ANPModel(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=2, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention(
      (linear_layers_list): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (linear_layers_list2): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (multihead_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
      )
    )
  )
  (_decoder): ANPDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=129, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
Num parameters: 199426
Namespace(dataset='XAUUSD', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='ANP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=1)
Saved Details
ANPModel(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=2, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention(
      (linear_layers_list): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (linear_layers_list2): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (multihead_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
      )
    )
  )
  (_decoder): ANPDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=129, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
Num parameters: 199426
Namespace(dataset='XAUUSD', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='ANP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=1)
Saved Details
ANPModel(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=2, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention(
      (linear_layers_list): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (linear_layers_list2): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (multihead_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
      )
    )
  )
  (_decoder): ANPDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=129, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
Num parameters: 199426
Namespace(dataset='XAUUSD', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='ANP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=1)
Saved Details
ANPModel(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=2, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention(
      (linear_layers_list): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (linear_layers_list2): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (multihead_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
      )
    )
  )
  (_decoder): ANPDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=129, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
Num parameters: 199426
Namespace(dataset='XAUUSD', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='ANP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=1)
Saved Details
ANPModel(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=2, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention(
      (linear_layers_list): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (linear_layers_list2): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (multihead_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
      )
    )
  )
  (_decoder): ANPDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=129, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
Num parameters: 199426
Namespace(dataset='XAUUSD', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='ANP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=1)
Saved Details
ANPModel(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=2, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention(
      (linear_layers_list): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (linear_layers_list2): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (multihead_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
      )
    )
  )
  (_decoder): ANPDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=129, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
Num parameters: 199426
Namespace(dataset='XAUUSD', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='ANP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=1)
Saved Details
ANPModel(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=2, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention(
      (linear_layers_list): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (linear_layers_list2): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (multihead_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
      )
    )
  )
  (_decoder): ANPDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=129, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
Num parameters: 199426
Namespace(dataset='XAUUSD', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='ANP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=1)
Saved Details
ANPModel(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=2, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention(
      (linear_layers_list): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (linear_layers_list2): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (multihead_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
      )
    )
  )
  (_decoder): ANPDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=129, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
Num parameters: 199426
Namespace(dataset='XAUUSD', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='ANP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=1)
Saved Details
ANPModel(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=2, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention(
      (linear_layers_list): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (linear_layers_list2): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (multihead_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
      )
    )
  )
  (_decoder): ANPDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=129, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
Num parameters: 199426
Namespace(dataset='XAUUSD', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='ANP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=1)
Saved Details
ANPModel(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=2, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention(
      (linear_layers_list): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (linear_layers_list2): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (multihead_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
      )
    )
  )
  (_decoder): ANPDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=129, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
Num parameters: 199426
Namespace(dataset='XAUUSD', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='ANP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=1)
Saved Details
ANPModel(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=2, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention(
      (linear_layers_list): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (linear_layers_list2): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (multihead_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
      )
    )
  )
  (_decoder): ANPDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=129, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
Num parameters: 199426
Namespace(dataset='XAUUSD', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='ANP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=1)
Saved Details
ANPModel(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=2, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention(
      (linear_layers_list): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (linear_layers_list2): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (multihead_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
      )
    )
  )
  (_decoder): ANPDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=129, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
Num parameters: 199426
Namespace(dataset='XAUUSD', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='ANP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=1)
Saved Details
ANPModel(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=2, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention(
      (linear_layers_list): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (linear_layers_list2): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (multihead_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
      )
    )
  )
  (_decoder): ANPDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=129, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
Num parameters: 199426
Namespace(dataset='XAUUSD', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='ANP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=1)
Saved Details
ANPModel(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=2, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention(
      (linear_layers_list): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (linear_layers_list2): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (multihead_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
      )
    )
  )
  (_decoder): ANPDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=129, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
Num parameters: 199426
Namespace(dataset='XAUUSD', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='ANP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=1)
Saved Details
ANPModel(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=2, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention(
      (linear_layers_list): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (linear_layers_list2): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (multihead_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
      )
    )
  )
  (_decoder): ANPDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=129, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
Num parameters: 199426
Namespace(dataset='XAUUSD', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='ANP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=1)
Saved Details
ANPModel(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=2, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention(
      (linear_layers_list): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (linear_layers_list2): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (multihead_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
      )
    )
  )
  (_decoder): ANPDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=129, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
Num parameters: 199426
Namespace(dataset='XAUUSD', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='ANP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=1)
Saved Details
ANPModel(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=2, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention(
      (linear_layers_list): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (linear_layers_list2): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (multihead_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
      )
    )
  )
  (_decoder): ANPDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=129, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
Num parameters: 199426
Namespace(dataset='XAUUSD', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='ANP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=1)
Saved Details
ANPModel(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=2, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention(
      (linear_layers_list): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (linear_layers_list2): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (multihead_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
      )
    )
  )
  (_decoder): ANPDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=129, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
Num parameters: 199426
Namespace(dataset='XAUUSD', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='ANP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=1)
Saved Details
ANPModel(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=2, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention(
      (linear_layers_list): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (linear_layers_list2): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (multihead_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
      )
    )
  )
  (_decoder): ANPDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=129, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
Num parameters: 199426
Namespace(dataset='XAUUSD', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='ANP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=1)
Saved Details
ANPModel(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=2, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention(
      (linear_layers_list): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (linear_layers_list2): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (multihead_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
      )
    )
  )
  (_decoder): ANPDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=129, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
Num parameters: 199426
Namespace(dataset='XAUUSD', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='ANP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=1)
Saved Details
ANPModel(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=2, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention(
      (linear_layers_list): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (linear_layers_list2): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (multihead_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
      )
    )
  )
  (_decoder): ANPDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=129, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
Num parameters: 199426
Namespace(dataset='XAUUSD', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='ANP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=1)
Saved Details
ANPModel(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=2, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention(
      (linear_layers_list): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (linear_layers_list2): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (multihead_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
      )
    )
  )
  (_decoder): ANPDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=129, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
Num parameters: 199426
Namespace(dataset='XAUUSD', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='ANP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=1)
Saved Details
ANPModel(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=2, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention(
      (linear_layers_list): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (linear_layers_list2): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (multihead_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
      )
    )
  )
  (_decoder): ANPDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=129, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
Num parameters: 199426
Namespace(dataset='XAUUSD', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='ANP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=1)
Saved Details
ANPModel(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=2, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention(
      (linear_layers_list): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (linear_layers_list2): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (multihead_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
      )
    )
  )
  (_decoder): ANPDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=129, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
Num parameters: 199426
Namespace(dataset='XAUUSD', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='ANP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=1)
Saved Details
ANPModel(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=2, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention(
      (linear_layers_list): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (linear_layers_list2): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (multihead_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
      )
    )
  )
  (_decoder): ANPDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=129, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
Num parameters: 199426
Namespace(dataset='XAUUSD', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='ANP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=1)
Saved Details
ANPModel(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=2, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention(
      (linear_layers_list): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (linear_layers_list2): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (multihead_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
      )
    )
  )
  (_decoder): ANPDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=129, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
Num parameters: 199426
Namespace(dataset='XAUUSD', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='ANP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=1)
Saved Details
ANPModel(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=2, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention(
      (linear_layers_list): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (linear_layers_list2): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (multihead_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
      )
    )
  )
  (_decoder): ANPDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=129, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
Num parameters: 199426
Namespace(dataset='XAUUSD', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='ANP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=1)
Saved Details
ANPModel(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=2, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention(
      (linear_layers_list): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (linear_layers_list2): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (multihead_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
      )
    )
  )
  (_decoder): ANPDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=129, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
Num parameters: 199426
Namespace(dataset='XAUUSD', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='ANP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=1)
Saved Details
ANPModel(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=2, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention(
      (linear_layers_list): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (linear_layers_list2): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (multihead_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
      )
    )
  )
  (_decoder): ANPDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=129, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
Num parameters: 199426
Namespace(dataset='XAUUSD', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='ANP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=1)
Saved Details
ANPModel(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=2, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention(
      (linear_layers_list): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (linear_layers_list2): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (multihead_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
      )
    )
  )
  (_decoder): ANPDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=129, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
Num parameters: 199426
Namespace(dataset='XAUUSD', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='ANP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=1)
Saved Details
ANPModel(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=2, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention(
      (linear_layers_list): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (linear_layers_list2): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (multihead_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
      )
    )
  )
  (_decoder): ANPDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=129, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
Num parameters: 199426
Namespace(dataset='XAUUSD', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='ANP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=1)
Saved Details
ANPModel(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=2, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention(
      (linear_layers_list): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (linear_layers_list2): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (multihead_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
      )
    )
  )
  (_decoder): ANPDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=129, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
Num parameters: 199426
Namespace(dataset='XAUUSD', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='ANP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=1)
Saved Details
ANPModel(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=2, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention(
      (linear_layers_list): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (linear_layers_list2): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (multihead_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
      )
    )
  )
  (_decoder): ANPDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=129, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
Num parameters: 199426
Namespace(dataset='XAUUSD', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='ANP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=1)
Saved Details
ANPModel(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=2, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention(
      (linear_layers_list): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (linear_layers_list2): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (multihead_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
      )
    )
  )
  (_decoder): ANPDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=129, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
Num parameters: 199426
Namespace(dataset='XAUUSD', seed=0, training_iterations=20000, num_epochs=10, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='ANP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=1)
Saved Details
ANPModel(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=2, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention(
      (linear_layers_list): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (linear_layers_list2): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (multihead_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
      )
    )
  )
  (_decoder): ANPDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=129, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
Num parameters: 199426
Namespace(dataset='XAUUSD', seed=0, training_iterations=20000, num_epochs=10, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='ANP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=1)
Saved Details
ANPModel(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=2, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention(
      (linear_layers_list): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (linear_layers_list2): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (multihead_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
      )
    )
  )
  (_decoder): ANPDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=129, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
Num parameters: 199426
Namespace(dataset='XAUUSD', seed=0, training_iterations=20000, num_epochs=10, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='ANP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=1)
Saved Details
ANPModel(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=2, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention(
      (linear_layers_list): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (linear_layers_list2): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (multihead_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
      )
    )
  )
  (_decoder): ANPDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=129, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
Num parameters: 199426
Namespace(dataset='XAUUSD', seed=0, training_iterations=20000, num_epochs=10, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='ANP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=1)
Saved Details
ANPModel(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=2, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention(
      (linear_layers_list): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (linear_layers_list2): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (multihead_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
      )
    )
  )
  (_decoder): ANPDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=129, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
Num parameters: 199426
Namespace(dataset='XAUUSD', seed=0, training_iterations=20000, num_epochs=10, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='ANP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=1)
Saved Details
ANPModel(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=2, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention(
      (linear_layers_list): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (linear_layers_list2): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (multihead_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
      )
    )
  )
  (_decoder): ANPDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=129, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
Num parameters: 199426
Namespace(dataset='XAUUSD', seed=0, training_iterations=20000, num_epochs=10, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='ANP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=1)
Saved Details
ANPModel(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=2, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention(
      (linear_layers_list): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (linear_layers_list2): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (multihead_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
      )
    )
  )
  (_decoder): ANPDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=129, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
Num parameters: 199426
Namespace(dataset='XAUUSD', seed=0, training_iterations=20000, num_epochs=10, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='ANP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=1)
Saved Details
ANPModel(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=2, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention(
      (linear_layers_list): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (linear_layers_list2): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (multihead_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
      )
    )
  )
  (_decoder): ANPDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=129, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
Num parameters: 199426
Namespace(dataset='XAUUSD', seed=0, training_iterations=20000, num_epochs=10, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='ANP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=1)
Saved Details
ANPModel(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=2, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention(
      (linear_layers_list): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (linear_layers_list2): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (multihead_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
      )
    )
  )
  (_decoder): ANPDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=129, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
Num parameters: 199426
Namespace(dataset='XAUUSD', seed=0, training_iterations=200, num_epochs=10, test_1d_every=20, save_results_every=1, num_test_tasks=20, max_context_points=50, model_type='ANP', attention_type='multihead', gpu_id=0, epochs=10, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=1)
Saved Details
ANPModel(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=2, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention(
      (linear_layers_list): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (linear_layers_list2): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (multihead_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
      )
    )
  )
  (_decoder): ANPDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=129, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
Num parameters: 199426
Namespace(dataset='XAUUSD', seed=0, training_iterations=200, num_epochs=10, test_1d_every=20, save_results_every=1, num_test_tasks=20, max_context_points=50, model_type='ANP', attention_type='multihead', gpu_id=0, epochs=10, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=1)
Saved Details
ANPModel(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=2, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention(
      (linear_layers_list): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (linear_layers_list2): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (multihead_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
      )
    )
  )
  (_decoder): ANPDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=129, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
Num parameters: 199426
Namespace(dataset='XAUUSD', seed=0, training_iterations=200, num_epochs=10, test_1d_every=20, save_results_every=1, num_test_tasks=20, max_context_points=50, model_type='ANP', attention_type='multihead', gpu_id=0, epochs=10, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=1)
Saved Details
ANPModel(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=2, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention(
      (linear_layers_list): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (linear_layers_list2): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (multihead_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
      )
    )
  )
  (_decoder): ANPDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=129, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
Num parameters: 199426
Namespace(dataset='XAUUSD', seed=0, training_iterations=200, num_epochs=10, test_1d_every=20, save_results_every=1, num_test_tasks=20, max_context_points=50, model_type='ANP', attention_type='multihead', gpu_id=0, epochs=10, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=1)
Saved Details
ANPModel(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=2, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention(
      (linear_layers_list): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (linear_layers_list2): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (multihead_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
      )
    )
  )
  (_decoder): ANPDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=129, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
Num parameters: 199426
Namespace(dataset='XAUUSD', seed=0, training_iterations=200, num_epochs=10, test_1d_every=20, save_results_every=1, num_test_tasks=20, max_context_points=10, model_type='ANP', attention_type='multihead', gpu_id=0, epochs=10, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=1)
Saved Details
ANPModel(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=2, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention(
      (linear_layers_list): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (linear_layers_list2): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (multihead_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
      )
    )
  )
  (_decoder): ANPDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=129, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
Num parameters: 199426
Namespace(dataset='XAUUSD', seed=0, training_iterations=200, num_epochs=10, test_1d_every=20, save_results_every=1, num_test_tasks=20, max_context_points=10, model_type='ANP', attention_type='multihead', gpu_id=0, epochs=10, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=1)
Saved Details
ANPModel(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=2, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention(
      (linear_layers_list): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (linear_layers_list2): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (multihead_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
      )
    )
  )
  (_decoder): ANPDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=129, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
Num parameters: 199426
Namespace(dataset='XAUUSD', seed=0, training_iterations=200, num_epochs=10, test_1d_every=20, save_results_every=1, num_test_tasks=20, max_context_points=10, model_type='ANP', attention_type='multihead', gpu_id=0, epochs=10, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=1)
Saved Details
ANPModel(
  (_latent_encoder): ANPLatentEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=2, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_latent): ContextToLatentDistribution(
      (mean_layer): Linear(in_features=128, out_features=128, bias=True)
      (log_variance_layer): Linear(in_features=128, out_features=128, bias=True)
    )
  )
  (_decoder): ANPDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=129, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
Num parameters: 132866
Namespace(dataset='XAUUSD', seed=0, training_iterations=200, num_epochs=10, test_1d_every=20, save_results_every=1, num_test_tasks=20, max_context_points=10, model_type='ANP', attention_type='multihead', gpu_id=0, epochs=10, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=False, use_latent_path=True, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=1)
Saved Details
ANPModel(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=2, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention()
  )
  (_decoder): ANPDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=129, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
Num parameters: 99842
Namespace(dataset='gp', seed=0, training_iterations=200, num_epochs=10, test_1d_every=20, save_results_every=1, num_test_tasks=20, max_context_points=10, model_type='CNP', attention_type='multihead', gpu_id=0, epochs=10, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=1)
Saved Details
ANPModel(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=2, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention(
      (linear_layers_list): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (linear_layers_list2): ModuleList(
        (0): Linear(in_features=1, out_features=128, bias=True)
        (1): Linear(in_features=128, out_features=128, bias=True)
      )
      (multihead_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
      )
    )
  )
  (_latent_encoder): ANPLatentEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=2, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_latent): ContextToLatentDistribution(
      (mean_layer): Linear(in_features=128, out_features=128, bias=True)
      (log_variance_layer): Linear(in_features=128, out_features=128, bias=True)
    )
  )
  (_decoder): ANPDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=257, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)
Num parameters: 315266
Namespace(dataset='XAUUSD', seed=0, training_iterations=200, num_epochs=10, test_1d_every=20, save_results_every=1, num_test_tasks=20, max_context_points=10, model_type='ANP', attention_type='multihead', gpu_id=0, epochs=10, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=True, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=1)
Saved Details
Evd_det_model(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=2, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention()
  )
  (_evidential_decoder): ANPEvidentialDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=129, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
    )
    (transform_gamma): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_v): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_alpha): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_beta): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
  )
)
Num parameters: 132868
Namespace(dataset='xauusd', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='CNP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=1)
Saved Details
Evd_det_model(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=2, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention()
  )
  (_evidential_decoder): ANPEvidentialDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=129, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
    )
    (transform_gamma): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_v): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_alpha): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_beta): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
  )
)
Num parameters: 132868
Namespace(dataset='xauusd', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='CNP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=1)
Saved Details
Evd_det_model(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=2, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention()
  )
  (_evidential_decoder): ANPEvidentialDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=129, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
    )
    (transform_gamma): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_v): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_alpha): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_beta): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
  )
)
Num parameters: 132868
Namespace(dataset='xauusd', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='CNP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=1)
Saved Details
Evd_det_model(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=2, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention()
  )
  (_evidential_decoder): ANPEvidentialDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=129, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
    )
    (transform_gamma): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_v): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_alpha): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_beta): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
  )
)
Num parameters: 132868
Namespace(dataset='xauusd', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=150, model_type='CNP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=1)
Saved Details
Evd_det_model(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=2, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention()
  )
  (_evidential_decoder): ANPEvidentialDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=129, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
    )
    (transform_gamma): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_v): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_alpha): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_beta): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
  )
)
Num parameters: 132868
Namespace(dataset='xauusd', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=150, model_type='CNP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=1)
Saved Details
Evd_det_model(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=2, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention()
  )
  (_evidential_decoder): ANPEvidentialDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=129, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
    )
    (transform_gamma): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_v): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_alpha): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_beta): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
  )
)
Num parameters: 132868
Namespace(dataset='xauusd', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=150, model_type='CNP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=1)
Saved Details
Evd_det_model(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=2, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention()
  )
  (_evidential_decoder): ANPEvidentialDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=129, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
    )
    (transform_gamma): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_v): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_alpha): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_beta): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
  )
)
Num parameters: 132868
Namespace(dataset='xauusd', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=150, model_type='CNP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=1)
Saved Details
Evd_det_model(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=2, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention()
  )
  (_evidential_decoder): ANPEvidentialDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=129, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
    )
    (transform_gamma): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_v): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_alpha): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_beta): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
  )
)
Num parameters: 132868
Namespace(dataset='xauusd', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=150, model_type='CNP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=1)
Saved Details
Evd_det_model(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=2, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention()
  )
  (_evidential_decoder): ANPEvidentialDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=129, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
    )
    (transform_gamma): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_v): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_alpha): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_beta): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
  )
)
Num parameters: 132868
Namespace(dataset='xauusd', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=150, model_type='CNP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=1)
Saved Details
Evd_det_model(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=2, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention()
  )
  (_evidential_decoder): ANPEvidentialDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=129, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
    )
    (transform_gamma): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_v): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_alpha): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_beta): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
  )
)
Num parameters: 132868
Namespace(dataset='xauusd', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=150, model_type='CNP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=1)
Saved Details
Evd_det_model(
  (_deterministic_encoder): ANPDeterministicEncoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=2, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=128, bias=True)
    )
    (_attention): Attention()
  )
  (_evidential_decoder): ANPEvidentialDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=129, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
    )
    (transform_gamma): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_v): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_alpha): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_beta): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
  )
)
Num parameters: 132868
Namespace(dataset='xauusd', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='CNP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=1)
Saved Details
LSTM_Evd_Model(
  (_deterministic_encoder): LSTMModel(
    (lstm): LSTM(2, 128, batch_first=True)
    (batch_norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (fc): Linear(in_features=128, out_features=128, bias=True)
  )
  (_evidential_decoder): ANPEvidentialDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=129, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
    )
    (transform_gamma): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_v): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_alpha): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_beta): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
  )
)
Num parameters: 150788
Namespace(input_size=2, lstm_layers=1, lstm_dropout=0.4, dataset='xauusd', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='CNP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=1)
Saved Details
LSTM_Evd_Model(
  (_deterministic_encoder): LSTMModel(
    (lstm): LSTM(2, 128, batch_first=True)
    (batch_norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (fc): Linear(in_features=128, out_features=128, bias=True)
  )
  (_evidential_decoder): ANPEvidentialDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=129, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
    )
    (transform_gamma): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_v): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_alpha): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_beta): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
  )
)
Num parameters: 150788
Namespace(input_size=2, lstm_layers=1, lstm_dropout=0.4, dataset='xauusd', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='CNP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=1)
Saved Details
LSTM_Evd_Model(
  (_deterministic_encoder): LSTMModel(
    (lstm): LSTM(2, 128, batch_first=True)
    (batch_norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (fc): Linear(in_features=128, out_features=128, bias=True)
  )
  (_evidential_decoder): ANPEvidentialDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=129, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
    )
    (transform_gamma): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_v): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_alpha): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_beta): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
  )
)
Num parameters: 150788
Namespace(input_size=2, lstm_layers=1, lstm_dropout=0.4, dataset='xauusd', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='CNP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5, channels=1)
Saved Details
LSTM_Evd_Model(
  (_deterministic_encoder): LSTMModel(
    (lstm): LSTM(2, 32, batch_first=True)
    (batch_norm): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (fc): Linear(in_features=32, out_features=32, bias=True)
  )
  (_evidential_decoder): ANPEvidentialDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=129, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
    )
    (transform_gamma): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_v): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_alpha): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_beta): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
  )
)
Num parameters: 72164
Namespace(input_size=2, lstm_layers=1, lstm_hidden_size=32, lstm_dropout=0.4, channels=1, dataset='xauusd', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='CNP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5)
Saved Details
LSTM_Evd_Model(
  (_deterministic_encoder): LSTMModel(
    (lstm): LSTM(2, 32, batch_first=True)
    (batch_norm): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (fc): Linear(in_features=32, out_features=32, bias=True)
  )
  (_evidential_decoder): ANPEvidentialDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=129, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
    )
    (transform_gamma): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_v): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_alpha): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_beta): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
  )
)
Num parameters: 72164
Namespace(input_size=2, lstm_layers=1, lstm_hidden_size=32, lstm_dropout=0.4, channels=1, x_size=1, y_size=1, dataset='xauusd', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='CNP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5)
Saved Details
LSTM_Evd_Model(
  (_deterministic_encoder): LSTMModel(
    (lstm): LSTM(2, 32, batch_first=True)
    (fc): Linear(in_features=32, out_features=32, bias=True)
  )
  (_evidential_decoder): ANPEvidentialDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=129, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
    )
    (transform_gamma): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_v): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_alpha): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_beta): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
  )
)
Num parameters: 72100
Namespace(input_size=2, lstm_layers=1, lstm_hidden_size=32, lstm_dropout=0.4, channels=1, x_size=1, y_size=1, dataset='xauusd', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='CNP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5)
Saved Details
LSTM_Evd_Model(
  (_deterministic_encoder): LSTMModel(
    (lstm): LSTM(2, 32, batch_first=True)
    (fc): Linear(in_features=32, out_features=32, bias=True)
  )
  (_evidential_decoder): ANPEvidentialDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=33, out_features=64, bias=True)
    )
    (transform_gamma): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_v): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_alpha): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_beta): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
  )
)
Num parameters: 24740
Namespace(input_size=2, lstm_layers=1, lstm_hidden_size=32, lstm_dropout=0.4, channels=1, x_size=1, y_size=1, dataset='xauusd', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='CNP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5)
Saved Details
LSTM_Evd_Model(
  (_deterministic_encoder): LSTMModel(
    (lstm): LSTM(2, 32, batch_first=True)
    (fc): Linear(in_features=32, out_features=32, bias=True)
  )
  (_evidential_decoder): ANPEvidentialDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=33, out_features=64, bias=True)
    )
    (transform_gamma): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_v): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_alpha): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_beta): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
  )
)
Num parameters: 24740
Namespace(input_size=2, lstm_layers=1, lstm_hidden_size=32, lstm_dropout=0.4, channels=1, x_size=1, y_size=1, dataset='xauusd', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='CNP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5)
Saved Details
LSTM_Evd_Model(
  (_deterministic_encoder): LSTMModel(
    (lstm): LSTM(2, 32, batch_first=True)
    (fc): Linear(in_features=32, out_features=32, bias=True)
  )
  (_evidential_decoder): ANPEvidentialDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=33, out_features=64, bias=True)
    )
    (transform_gamma): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_v): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_alpha): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_beta): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
  )
)
Num parameters: 24740
Namespace(input_size=2, lstm_layers=1, lstm_hidden_size=32, lstm_dropout=0.4, channels=1, x_size=1, y_size=1, dataset='xauusd', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='CNP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=0.1, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5)
Saved Details
LSTM_Evd_Model(
  (_deterministic_encoder): LSTMModel(
    (lstm): LSTM(2, 32, batch_first=True)
    (fc): Linear(in_features=32, out_features=32, bias=True)
  )
  (_evidential_decoder): ANPEvidentialDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=33, out_features=64, bias=True)
    )
    (transform_gamma): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_v): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_alpha): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_beta): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
  )
)
Num parameters: 24740
Namespace(input_size=2, lstm_layers=1, lstm_hidden_size=32, lstm_dropout=0.4, channels=1, x_size=1, y_size=1, dataset='xauusd', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='CNP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=1.0, nig_nll_ker_reg_coef=2.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5)
Saved Details
LSTM_Evd_Model(
  (_deterministic_encoder): LSTMModel(
    (lstm): LSTM(2, 32, batch_first=True)
    (fc): Linear(in_features=32, out_features=32, bias=True)
  )
  (_evidential_decoder): ANPEvidentialDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=33, out_features=64, bias=True)
    )
    (transform_gamma): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_v): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_alpha): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_beta): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
  )
)
Num parameters: 24740
Namespace(input_size=2, lstm_layers=1, lstm_hidden_size=32, lstm_dropout=0.4, channels=1, x_size=1, y_size=1, dataset='xauusd', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='CNP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=2.0, nig_nll_ker_reg_coef=2.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5)
Saved Details
LSTM_Evd_Model(
  (_deterministic_encoder): LSTMModel(
    (lstm): LSTM(2, 32, batch_first=True)
    (fc): Linear(in_features=32, out_features=32, bias=True)
  )
  (_evidential_decoder): ANPEvidentialDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=33, out_features=64, bias=True)
    )
    (transform_gamma): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_v): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_alpha): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_beta): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
  )
)
Num parameters: 24740
Namespace(input_size=2, lstm_layers=1, lstm_hidden_size=32, lstm_dropout=0.4, channels=1, x_size=1, y_size=1, dataset='xauusd', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='CNP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=2.0, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5)
Saved Details
LSTM_Evd_Model(
  (_deterministic_encoder): LSTMModel(
    (lstm): LSTM(2, 32, batch_first=True)
    (fc): Linear(in_features=32, out_features=32, bias=True)
  )
  (_evidential_decoder): ANPEvidentialDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=33, out_features=64, bias=True)
    )
    (transform_gamma): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_v): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_alpha): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_beta): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
  )
)
Num parameters: 24740
Namespace(input_size=2, lstm_layers=1, lstm_hidden_size=32, lstm_dropout=0.4, channels=1, x_size=1, y_size=1, dataset='xauusd', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='CNP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=3.0, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5)
Saved Details
LSTM_Evd_Model(
  (_deterministic_encoder): LSTMModel(
    (lstm): LSTM(2, 32, batch_first=True)
    (fc): Linear(in_features=32, out_features=32, bias=True)
  )
  (_evidential_decoder): ANPEvidentialDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=33, out_features=64, bias=True)
    )
    (transform_gamma): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_v): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_alpha): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_beta): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
  )
)
Num parameters: 24740
Namespace(input_size=2, lstm_layers=1, lstm_hidden_size=32, lstm_dropout=0.4, channels=1, x_size=1, y_size=1, dataset='xauusd', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='CNP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=1.0, nig_nll_ker_reg_coef=1.0, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5)
Saved Details
LSTM_Evd_Model(
  (_deterministic_encoder): LSTMModel(
    (lstm): LSTM(2, 64, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)
    (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    (attention): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
    )
    (fc): Sequential(
      (0): Linear(in_features=128, out_features=64, bias=True)
      (1): ReLU()
      (2): Dropout(p=0.3, inplace=False)
      (3): Linear(in_features=64, out_features=64, bias=True)
    )
  )
  (_evidential_decoder): ANPEvidentialDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=129, out_features=64, bias=True)
    )
    (transform_gamma): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_v): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_alpha): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_beta): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
  )
)
Num parameters: 238084
Namespace(input_size=2, lstm_layers=2, lstm_hidden_size=64, lstm_dropout=0.3, channels=1, x_size=1, y_size=1, dataset='xauusd', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='CNP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=2.0, nig_nll_ker_reg_coef=0.5, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5)
Saved Details
LSTM_Evd_Model(
  (_deterministic_encoder): LSTMModel(
    (lstm): LSTM(2, 64, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)
    (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    (attention): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
    )
    (fc): Sequential(
      (0): Linear(in_features=128, out_features=64, bias=True)
      (1): ReLU()
      (2): Dropout(p=0.3, inplace=False)
      (3): Linear(in_features=64, out_features=64, bias=True)
    )
  )
  (_evidential_decoder): ANPEvidentialDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=129, out_features=64, bias=True)
    )
    (transform_gamma): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_v): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_alpha): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_beta): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
  )
)
Num parameters: 238084
Namespace(input_size=2, lstm_layers=2, lstm_hidden_size=64, lstm_dropout=0.3, channels=1, x_size=1, y_size=1, dataset='xauusd', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='CNP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=2.0, nig_nll_ker_reg_coef=0.5, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5)
Saved Details
LSTM_Evd_Model(
  (_deterministic_encoder): LSTMModel(
    (lstm): LSTM(2, 64, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)
    (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    (attention): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
    )
    (fc): Sequential(
      (0): Linear(in_features=128, out_features=64, bias=True)
      (1): ReLU()
      (2): Dropout(p=0.3, inplace=False)
      (3): Linear(in_features=64, out_features=64, bias=True)
    )
  )
  (_evidential_decoder): ANPEvidentialDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=129, out_features=64, bias=True)
    )
    (transform_gamma): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_v): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_alpha): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_beta): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
  )
)
Num parameters: 238084
Namespace(input_size=2, lstm_layers=2, lstm_hidden_size=64, lstm_dropout=0.3, channels=1, x_size=1, y_size=1, dataset='xauusd', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='CNP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=2.0, nig_nll_ker_reg_coef=0.5, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5)
Saved Details
LSTM_Evd_Model(
  (_deterministic_encoder): LSTMModel(
    (lstm): LSTM(2, 64, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)
    (fc): Linear(in_features=64, out_features=64, bias=True)
  )
  (_evidential_decoder): ANPEvidentialDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=65, out_features=64, bias=True)
    )
    (transform_gamma): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_v): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_alpha): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_beta): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
  )
)
Num parameters: 159428
Namespace(input_size=2, lstm_layers=2, lstm_hidden_size=64, lstm_dropout=0.3, channels=1, x_size=1, y_size=1, dataset='xauusd', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='CNP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=2.0, nig_nll_ker_reg_coef=0.5, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5)
Saved Details
LSTM_Evd_Model(
  (_deterministic_encoder): LSTMModel(
    (lstm): LSTM(2, 64, num_layers=2, batch_first=True, dropout=0.3)
    (fc): Linear(in_features=64, out_features=64, bias=True)
  )
  (_evidential_decoder): ANPEvidentialDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=65, out_features=64, bias=True)
    )
    (transform_gamma): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_v): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_alpha): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_beta): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
  )
)
Num parameters: 75972
Namespace(input_size=2, lstm_layers=2, lstm_hidden_size=64, lstm_dropout=0.3, channels=1, x_size=1, y_size=1, dataset='xauusd', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='CNP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=2.0, nig_nll_ker_reg_coef=0.5, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5)
Saved Details
LSTM_Evd_Model(
  (_deterministic_encoder): LSTMModel(
    (lstm): LSTM(2, 64, num_layers=2, batch_first=True, dropout=0.3)
    (fc): Linear(in_features=64, out_features=64, bias=True)
  )
  (_evidential_decoder): ANPEvidentialDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=65, out_features=64, bias=True)
    )
    (transform_gamma): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_v): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_alpha): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_beta): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
  )
)
Num parameters: 75972
Namespace(input_size=2, lstm_layers=2, lstm_hidden_size=64, lstm_dropout=0.3, channels=1, x_size=1, y_size=1, dataset='xauusd', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='CNP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=2.0, nig_nll_ker_reg_coef=0.5, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5)
Saved Details
LSTM_Evd_Model(
  (_deterministic_encoder): LSTMModel(
    (lstm): LSTM(2, 64, num_layers=2, batch_first=True, dropout=0.3)
    (fc): Linear(in_features=64, out_features=64, bias=True)
  )
  (_evidential_decoder): ANPEvidentialDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=65, out_features=64, bias=True)
    )
    (transform_gamma): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_v): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_alpha): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_beta): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
  )
)
Num parameters: 75972
Namespace(input_size=2, lstm_layers=2, lstm_hidden_size=64, lstm_dropout=0.3, channels=1, x_size=1, y_size=1, dataset='xauusd', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='CNP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=2.0, nig_nll_ker_reg_coef=0.5, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5)
Saved Details
LSTM_Evd_Model(
  (_deterministic_encoder): LSTMModel(
    (lstm): LSTM(2, 64, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)
    (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    (attention): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
    )
    (fc): Sequential(
      (0): Linear(in_features=128, out_features=64, bias=True)
      (1): ReLU()
      (2): Dropout(p=0.3, inplace=False)
      (3): Linear(in_features=64, out_features=64, bias=True)
    )
  )
  (_evidential_decoder): ANPEvidentialDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=65, out_features=64, bias=True)
    )
    (transform_gamma): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_v): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_alpha): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_beta): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
  )
)
Num parameters: 233988
Namespace(input_size=2, lstm_layers=2, lstm_hidden_size=64, lstm_dropout=0.3, channels=1, x_size=1, y_size=1, dataset='xauusd', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='CNP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=2.0, nig_nll_ker_reg_coef=0.5, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5)
Saved Details
LSTM_Evd_Model(
  (_deterministic_encoder): LSTMModel(
    (lstm): LSTM(2, 64, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)
    (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    (attention): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
    )
    (fc): Sequential(
      (0): Linear(in_features=128, out_features=64, bias=True)
      (1): ReLU()
      (2): Dropout(p=0.3, inplace=False)
      (3): Linear(in_features=64, out_features=64, bias=True)
    )
  )
  (_evidential_decoder): ANPEvidentialDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=65, out_features=64, bias=True)
    )
    (transform_gamma): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_v): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_alpha): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_beta): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
  )
)
Num parameters: 233988
Namespace(input_size=2, lstm_layers=2, lstm_hidden_size=64, lstm_dropout=0.3, channels=1, x_size=1, y_size=1, dataset='xauusd', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='CNP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=2.0, nig_nll_ker_reg_coef=0.5, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5)
Saved Details
Transformer_Evd_Model(
  (_deterministic_encoder): TransformerModel(
    (input_projection): Linear(in_features=8, out_features=64, bias=True)
    (transformer_encoder): TransformerEncoder(
      (layers): ModuleList(
        (0-1): 2 x TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=256, bias=True)
          (dropout): Dropout(p=0.3, inplace=False)
          (linear2): Linear(in_features=256, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.3, inplace=False)
          (dropout2): Dropout(p=0.3, inplace=False)
        )
      )
    )
    (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (fc): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Dropout(p=0.3, inplace=False)
      (3): Linear(in_features=64, out_features=64, bias=True)
    )
  )
  (_evidential_decoder): ANPEvidentialDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=68, out_features=64, bias=True)
    )
    (transform_gamma): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_v): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_alpha): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_beta): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
  )
)
Num parameters: 130308
Namespace(input_size=8, lstm_layers=2, lstm_hidden_size=64, lstm_dropout=0.3, channels=1, x_size=4, y_size=1, dataset='xauusd', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='CNP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=2.0, nig_nll_ker_reg_coef=0.5, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5)
Saved Details
Transformer_Evd_Model(
  (_deterministic_encoder): TransformerModel(
    (input_projection): Linear(in_features=8, out_features=64, bias=True)
    (transformer_encoder): TransformerEncoder(
      (layers): ModuleList(
        (0-1): 2 x TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=256, bias=True)
          (dropout): Dropout(p=0.3, inplace=False)
          (linear2): Linear(in_features=256, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.3, inplace=False)
          (dropout2): Dropout(p=0.3, inplace=False)
        )
      )
    )
    (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (fc): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Dropout(p=0.3, inplace=False)
      (3): Linear(in_features=64, out_features=64, bias=True)
    )
  )
  (_evidential_decoder): ANPEvidentialDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=68, out_features=64, bias=True)
    )
    (transform_gamma): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_v): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_alpha): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_beta): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
  )
)
Num parameters: 130308
Namespace(input_size=8, lstm_layers=2, lstm_hidden_size=64, lstm_dropout=0.3, channels=1, x_size=4, y_size=1, dataset='xauusd', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='CNP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=2.0, nig_nll_ker_reg_coef=0.5, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5)
Saved Details
Transformer_Evd_Model(
  (_deterministic_encoder): TransformerModel(
    (input_projection): Linear(in_features=8, out_features=64, bias=True)
    (transformer_encoder): TransformerEncoder(
      (layers): ModuleList(
        (0-1): 2 x TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=256, bias=True)
          (dropout): Dropout(p=0.3, inplace=False)
          (linear2): Linear(in_features=256, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.3, inplace=False)
          (dropout2): Dropout(p=0.3, inplace=False)
        )
      )
    )
    (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (fc): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Dropout(p=0.3, inplace=False)
      (3): Linear(in_features=64, out_features=64, bias=True)
    )
  )
  (_evidential_decoder): ANPEvidentialDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=68, out_features=64, bias=True)
    )
    (transform_gamma): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_v): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_alpha): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_beta): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
  )
)
Num parameters: 130308
Namespace(input_size=8, lstm_layers=2, lstm_hidden_size=64, lstm_dropout=0.3, channels=1, x_size=4, y_size=1, dataset='xauusd', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='CNP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=2.0, nig_nll_ker_reg_coef=0.5, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5)
Saved Details
Transformer_Evd_Model(
  (_deterministic_encoder): TransformerModel(
    (input_projection): Linear(in_features=8, out_features=64, bias=True)
    (transformer_encoder): TransformerEncoder(
      (layers): ModuleList(
        (0-1): 2 x TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=256, bias=True)
          (dropout): Dropout(p=0.3, inplace=False)
          (linear2): Linear(in_features=256, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.3, inplace=False)
          (dropout2): Dropout(p=0.3, inplace=False)
        )
      )
    )
    (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (fc): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Dropout(p=0.3, inplace=False)
      (3): Linear(in_features=64, out_features=64, bias=True)
    )
  )
  (_evidential_decoder): ANPEvidentialDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=68, out_features=64, bias=True)
    )
    (transform_gamma): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_v): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_alpha): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_beta): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
  )
)
Num parameters: 130308
Namespace(input_size=8, lstm_layers=2, lstm_hidden_size=64, lstm_dropout=0.3, channels=1, x_size=4, y_size=1, dataset='xauusd', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='CNP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=2.0, nig_nll_ker_reg_coef=0.5, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5)
Saved Details
Transformer_Evd_Model(
  (_deterministic_encoder): TransformerModel(
    (input_projection): Linear(in_features=8, out_features=64, bias=True)
    (transformer_encoder): TransformerEncoder(
      (layers): ModuleList(
        (0-1): 2 x TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=256, bias=True)
          (dropout): Dropout(p=0.3, inplace=False)
          (linear2): Linear(in_features=256, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.3, inplace=False)
          (dropout2): Dropout(p=0.3, inplace=False)
        )
      )
    )
    (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (fc): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Dropout(p=0.3, inplace=False)
      (3): Linear(in_features=64, out_features=64, bias=True)
    )
  )
  (_evidential_decoder): ANPEvidentialDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=68, out_features=64, bias=True)
    )
    (transform_gamma): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_v): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_alpha): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_beta): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
  )
)
Num parameters: 130308
Namespace(input_size=8, lstm_layers=2, lstm_hidden_size=64, lstm_dropout=0.3, channels=1, x_size=4, y_size=1, dataset='xauusd', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='CNP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=2.0, nig_nll_ker_reg_coef=0.5, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5)
Saved Details
Transformer_Evd_Model(
  (_deterministic_encoder): TransformerModel(
    (input_projection): Linear(in_features=8, out_features=64, bias=True)
    (transformer_encoder): TransformerEncoder(
      (layers): ModuleList(
        (0-1): 2 x TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=256, bias=True)
          (dropout): Dropout(p=0.3, inplace=False)
          (linear2): Linear(in_features=256, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.3, inplace=False)
          (dropout2): Dropout(p=0.3, inplace=False)
        )
      )
    )
    (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (fc): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Dropout(p=0.3, inplace=False)
      (3): Linear(in_features=64, out_features=64, bias=True)
    )
  )
  (_evidential_decoder): ANPEvidentialDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=68, out_features=64, bias=True)
    )
    (transform_gamma): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_v): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_alpha): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_beta): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
  )
)
Num parameters: 130308
Namespace(input_size=8, lstm_layers=2, lstm_hidden_size=64, lstm_dropout=0.3, channels=1, x_size=4, y_size=1, dataset='xauusd', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='CNP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=2.0, nig_nll_ker_reg_coef=0.5, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5)
Saved Details
Transformer_Evd_Model(
  (_deterministic_encoder): TransformerModel(
    (input_projection): Linear(in_features=8, out_features=64, bias=True)
    (transformer_encoder): TransformerEncoder(
      (layers): ModuleList(
        (0-1): 2 x TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=256, bias=True)
          (dropout): Dropout(p=0.3, inplace=False)
          (linear2): Linear(in_features=256, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.3, inplace=False)
          (dropout2): Dropout(p=0.3, inplace=False)
        )
      )
    )
    (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (fc): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Dropout(p=0.3, inplace=False)
      (3): Linear(in_features=64, out_features=64, bias=True)
    )
  )
  (_evidential_decoder): ANPEvidentialDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=68, out_features=64, bias=True)
    )
    (transform_gamma): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_v): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_alpha): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_beta): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
  )
)
Num parameters: 130308
Namespace(input_size=8, lstm_layers=2, lstm_hidden_size=64, lstm_dropout=0.3, channels=1, x_size=4, y_size=1, dataset='xauusd', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='CNP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=2.0, nig_nll_ker_reg_coef=0.5, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5)
Saved Details
Transformer_Evd_Model(
  (_deterministic_encoder): TransformerModel(
    (input_projection): Linear(in_features=8, out_features=64, bias=True)
    (transformer_encoder): TransformerEncoder(
      (layers): ModuleList(
        (0-1): 2 x TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=256, bias=True)
          (dropout): Dropout(p=0.3, inplace=False)
          (linear2): Linear(in_features=256, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.3, inplace=False)
          (dropout2): Dropout(p=0.3, inplace=False)
        )
      )
    )
    (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (fc): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Dropout(p=0.3, inplace=False)
      (3): Linear(in_features=64, out_features=64, bias=True)
    )
  )
  (_evidential_decoder): ANPEvidentialDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=68, out_features=64, bias=True)
    )
    (transform_gamma): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_v): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_alpha): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_beta): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
  )
)
Num parameters: 130308
Namespace(input_size=8, lstm_layers=2, lstm_hidden_size=64, lstm_dropout=0.3, channels=1, x_size=4, y_size=1, dataset='xauusd', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='CNP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=2.0, nig_nll_ker_reg_coef=0.5, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5)
Saved Details
Transformer_Evd_Model(
  (_deterministic_encoder): TransformerModel(
    (input_projection): Linear(in_features=8, out_features=64, bias=True)
    (transformer_encoder): TransformerEncoder(
      (layers): ModuleList(
        (0-1): 2 x TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=256, bias=True)
          (dropout): Dropout(p=0.3, inplace=False)
          (linear2): Linear(in_features=256, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.3, inplace=False)
          (dropout2): Dropout(p=0.3, inplace=False)
        )
      )
    )
    (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (fc): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Dropout(p=0.3, inplace=False)
      (3): Linear(in_features=64, out_features=64, bias=True)
    )
  )
  (_evidential_decoder): ANPEvidentialDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=68, out_features=64, bias=True)
    )
    (transform_gamma): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_v): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_alpha): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_beta): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
  )
)
Num parameters: 130308
Namespace(input_size=8, lstm_layers=2, lstm_hidden_size=64, lstm_dropout=0.3, channels=1, x_size=4, y_size=1, dataset='xauusd', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='CNP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=2.0, nig_nll_ker_reg_coef=0.5, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5)
Saved Details
Transformer_Evd_Model(
  (_deterministic_encoder): TransformerModel(
    (input_projection): Linear(in_features=8, out_features=64, bias=True)
    (transformer_encoder): TransformerEncoder(
      (layers): ModuleList(
        (0-1): 2 x TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=256, bias=True)
          (dropout): Dropout(p=0.3, inplace=False)
          (linear2): Linear(in_features=256, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.3, inplace=False)
          (dropout2): Dropout(p=0.3, inplace=False)
        )
      )
    )
    (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (fc): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Dropout(p=0.3, inplace=False)
      (3): Linear(in_features=64, out_features=64, bias=True)
    )
  )
  (_evidential_decoder): ANPEvidentialDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=68, out_features=64, bias=True)
    )
    (transform_gamma): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_v): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_alpha): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_beta): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
  )
)
Num parameters: 130308
Namespace(input_size=8, lstm_layers=2, lstm_hidden_size=64, lstm_dropout=0.3, channels=1, x_size=4, y_size=1, dataset='xauusd', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='CNP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=2.0, nig_nll_ker_reg_coef=0.5, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5)
Saved Details
Transformer_Evd_Model(
  (_deterministic_encoder): TransformerModel(
    (input_projection): Linear(in_features=8, out_features=64, bias=True)
    (transformer_encoder): TransformerEncoder(
      (layers): ModuleList(
        (0-1): 2 x TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=256, bias=True)
          (dropout): Dropout(p=0.3, inplace=False)
          (linear2): Linear(in_features=256, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.3, inplace=False)
          (dropout2): Dropout(p=0.3, inplace=False)
        )
      )
    )
    (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (fc): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Dropout(p=0.3, inplace=False)
      (3): Linear(in_features=64, out_features=64, bias=True)
    )
  )
  (_evidential_decoder): ANPEvidentialDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=68, out_features=64, bias=True)
    )
    (transform_gamma): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_v): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_alpha): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_beta): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
  )
)
Num parameters: 130308
Namespace(input_size=8, lstm_layers=2, lstm_hidden_size=64, lstm_dropout=0.3, channels=1, x_size=4, y_size=1, dataset='xauusd', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='CNP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=2.0, nig_nll_ker_reg_coef=0.5, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5)
Saved Details
Transformer_Evd_Model(
  (_deterministic_encoder): TransformerModel(
    (input_projection): Linear(in_features=5, out_features=64, bias=True)
    (transformer_encoder): TransformerEncoder(
      (layers): ModuleList(
        (0-1): 2 x TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=256, bias=True)
          (dropout): Dropout(p=0.3, inplace=False)
          (linear2): Linear(in_features=256, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.3, inplace=False)
          (dropout2): Dropout(p=0.3, inplace=False)
        )
      )
    )
    (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (fc): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Dropout(p=0.3, inplace=False)
      (3): Linear(in_features=64, out_features=64, bias=True)
    )
  )
  (_evidential_decoder): ANPEvidentialDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=68, out_features=64, bias=True)
    )
    (transform_gamma): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_v): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_alpha): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_beta): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
  )
)
Num parameters: 130116
Namespace(input_size=5, lstm_layers=2, lstm_hidden_size=64, lstm_dropout=0.3, channels=1, x_size=4, y_size=1, dataset='xauusd', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='CNP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=2.0, nig_nll_ker_reg_coef=0.5, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5)
Saved Details
Transformer_Evd_Model(
  (_deterministic_encoder): TransformerModel(
    (input_projection): Linear(in_features=5, out_features=64, bias=True)
    (transformer_encoder): TransformerEncoder(
      (layers): ModuleList(
        (0-1): 2 x TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=256, bias=True)
          (dropout): Dropout(p=0.3, inplace=False)
          (linear2): Linear(in_features=256, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.3, inplace=False)
          (dropout2): Dropout(p=0.3, inplace=False)
        )
      )
    )
    (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (fc): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Dropout(p=0.3, inplace=False)
      (3): Linear(in_features=64, out_features=64, bias=True)
    )
  )
  (_evidential_decoder): ANPEvidentialDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=68, out_features=64, bias=True)
    )
    (transform_gamma): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_v): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_alpha): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_beta): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
  )
)
Num parameters: 130116
Namespace(input_size=5, lstm_layers=2, lstm_hidden_size=64, lstm_dropout=0.3, channels=1, x_size=4, y_size=1, dataset='xauusd', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='CNP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=2.0, nig_nll_ker_reg_coef=0.5, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5)
Saved Details
Transformer_Evd_Model(
  (_deterministic_encoder): TransformerModel(
    (input_projection): Linear(in_features=5, out_features=64, bias=True)
    (transformer_encoder): TransformerEncoder(
      (layers): ModuleList(
        (0-1): 2 x TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=256, bias=True)
          (dropout): Dropout(p=0.3, inplace=False)
          (linear2): Linear(in_features=256, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.3, inplace=False)
          (dropout2): Dropout(p=0.3, inplace=False)
        )
      )
    )
    (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (fc): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Dropout(p=0.3, inplace=False)
      (3): Linear(in_features=64, out_features=64, bias=True)
    )
  )
  (_evidential_decoder): ANPEvidentialDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=65, out_features=64, bias=True)
    )
    (transform_gamma): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_v): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_alpha): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_beta): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
  )
)
Num parameters: 129924
Namespace(input_size=5, lstm_layers=2, lstm_hidden_size=64, lstm_dropout=0.3, channels=1, x_size=1, y_size=1, dataset='xauusd', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='CNP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=2.0, nig_nll_ker_reg_coef=0.5, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5)
Saved Details
Transformer_Evd_Model(
  (_deterministic_encoder): TransformerModel(
    (input_projection): Linear(in_features=5, out_features=64, bias=True)
    (transformer_encoder): TransformerEncoder(
      (layers): ModuleList(
        (0-1): 2 x TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=256, bias=True)
          (dropout): Dropout(p=0.3, inplace=False)
          (linear2): Linear(in_features=256, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.3, inplace=False)
          (dropout2): Dropout(p=0.3, inplace=False)
        )
      )
    )
    (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (fc): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Dropout(p=0.3, inplace=False)
      (3): Linear(in_features=64, out_features=64, bias=True)
    )
  )
  (_evidential_decoder): ANPEvidentialDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=65, out_features=64, bias=True)
    )
    (transform_gamma): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_v): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_alpha): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_beta): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
  )
)
Num parameters: 129924
Namespace(input_size=5, lstm_layers=2, lstm_hidden_size=64, lstm_dropout=0.3, channels=1, x_size=1, y_size=1, dataset='xauusd', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='CNP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=2.0, nig_nll_ker_reg_coef=0.5, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5)
Saved Details
Transformer_Evd_Model(
  (_deterministic_encoder): TransformerModel(
    (input_projection): Linear(in_features=5, out_features=64, bias=True)
    (transformer_encoder): TransformerEncoder(
      (layers): ModuleList(
        (0-1): 2 x TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=256, bias=True)
          (dropout): Dropout(p=0.3, inplace=False)
          (linear2): Linear(in_features=256, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.3, inplace=False)
          (dropout2): Dropout(p=0.3, inplace=False)
        )
      )
    )
    (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (fc): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Dropout(p=0.3, inplace=False)
      (3): Linear(in_features=64, out_features=64, bias=True)
    )
  )
  (_evidential_decoder): ANPEvidentialDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=65, out_features=64, bias=True)
    )
    (transform_gamma): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_v): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_alpha): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_beta): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
  )
)
Num parameters: 129924
Namespace(input_size=5, lstm_layers=2, lstm_hidden_size=64, lstm_dropout=0.3, channels=1, x_size=1, y_size=1, dataset='xauusd', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='CNP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=2.0, nig_nll_ker_reg_coef=0.5, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5)
Saved Details
Transformer_Evd_Model(
  (_deterministic_encoder): TransformerModel(
    (input_projection): Linear(in_features=5, out_features=64, bias=True)
    (transformer_encoder): TransformerEncoder(
      (layers): ModuleList(
        (0-1): 2 x TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=256, bias=True)
          (dropout): Dropout(p=0.3, inplace=False)
          (linear2): Linear(in_features=256, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.3, inplace=False)
          (dropout2): Dropout(p=0.3, inplace=False)
        )
      )
    )
    (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (fc): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Dropout(p=0.3, inplace=False)
      (3): Linear(in_features=64, out_features=64, bias=True)
    )
  )
  (_evidential_decoder): ANPEvidentialDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=65, out_features=64, bias=True)
    )
    (transform_gamma): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_v): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_alpha): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_beta): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
  )
)
Num parameters: 129924
Namespace(input_size=5, lstm_layers=2, lstm_hidden_size=64, lstm_dropout=0.3, channels=1, x_size=1, y_size=1, dataset='xauusd', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='CNP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=2.0, nig_nll_ker_reg_coef=0.5, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5)
Saved Details
Transformer_Evd_Model(
  (_deterministic_encoder): TransformerModel(
    (input_projection): Linear(in_features=5, out_features=64, bias=True)
    (transformer_encoder): TransformerEncoder(
      (layers): ModuleList(
        (0-1): 2 x TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=256, bias=True)
          (dropout): Dropout(p=0.3, inplace=False)
          (linear2): Linear(in_features=256, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.3, inplace=False)
          (dropout2): Dropout(p=0.3, inplace=False)
        )
      )
    )
    (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (fc): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Dropout(p=0.3, inplace=False)
      (3): Linear(in_features=64, out_features=64, bias=True)
    )
  )
  (_evidential_decoder): ANPEvidentialDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=65, out_features=64, bias=True)
    )
    (transform_gamma): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_v): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_alpha): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_beta): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
  )
)
Num parameters: 129924
Namespace(input_size=5, lstm_layers=2, lstm_hidden_size=64, lstm_dropout=0.3, channels=1, x_size=1, y_size=1, dataset='xauusd', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='CNP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=2.0, nig_nll_ker_reg_coef=0.5, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5)
Saved Details
Transformer_Evd_Model(
  (_deterministic_encoder): TransformerModel(
    (input_projection): Linear(in_features=5, out_features=64, bias=True)
    (transformer_encoder): TransformerEncoder(
      (layers): ModuleList(
        (0-1): 2 x TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=256, bias=True)
          (dropout): Dropout(p=0.3, inplace=False)
          (linear2): Linear(in_features=256, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.3, inplace=False)
          (dropout2): Dropout(p=0.3, inplace=False)
        )
      )
    )
    (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (fc): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Dropout(p=0.3, inplace=False)
      (3): Linear(in_features=64, out_features=64, bias=True)
    )
  )
  (_evidential_decoder): ANPEvidentialDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=65, out_features=64, bias=True)
    )
    (transform_gamma): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_v): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_alpha): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_beta): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
  )
)
Num parameters: 129924
Namespace(input_size=5, lstm_layers=2, lstm_hidden_size=64, lstm_dropout=0.3, channels=1, x_size=1, y_size=1, dataset='xauusd', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='CNP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=2.0, nig_nll_ker_reg_coef=0.5, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5)
Saved Details
Transformer_Evd_Model(
  (_deterministic_encoder): TransformerModel(
    (input_projection): Linear(in_features=5, out_features=64, bias=True)
    (transformer_encoder): TransformerEncoder(
      (layers): ModuleList(
        (0-1): 2 x TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=256, bias=True)
          (dropout): Dropout(p=0.3, inplace=False)
          (linear2): Linear(in_features=256, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.3, inplace=False)
          (dropout2): Dropout(p=0.3, inplace=False)
        )
      )
    )
    (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (fc): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Dropout(p=0.3, inplace=False)
      (3): Linear(in_features=64, out_features=64, bias=True)
    )
  )
  (_evidential_decoder): ANPEvidentialDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=67, out_features=64, bias=True)
    )
    (transform_gamma): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_v): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_alpha): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_beta): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
  )
)
Num parameters: 130052
Namespace(input_size=5, lstm_layers=2, lstm_hidden_size=64, lstm_dropout=0.3, channels=1, x_size=3, y_size=1, dataset='xauusd', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='CNP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=2.0, nig_nll_ker_reg_coef=0.5, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5)
Saved Details
Transformer_Evd_Model(
  (_deterministic_encoder): TransformerModel(
    (input_projection): Linear(in_features=5, out_features=64, bias=True)
    (transformer_encoder): TransformerEncoder(
      (layers): ModuleList(
        (0-1): 2 x TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=256, bias=True)
          (dropout): Dropout(p=0.3, inplace=False)
          (linear2): Linear(in_features=256, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.3, inplace=False)
          (dropout2): Dropout(p=0.3, inplace=False)
        )
      )
    )
    (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (fc): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Dropout(p=0.3, inplace=False)
      (3): Linear(in_features=64, out_features=64, bias=True)
    )
  )
  (_evidential_decoder): ANPEvidentialDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=67, out_features=64, bias=True)
    )
    (transform_gamma): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_v): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_alpha): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_beta): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
  )
)
Num parameters: 130052
Namespace(input_size=5, lstm_layers=2, lstm_hidden_size=64, lstm_dropout=0.3, channels=1, x_size=3, y_size=1, dataset='xauusd', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='CNP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=2.0, nig_nll_ker_reg_coef=0.5, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5)
Saved Details
Transformer_Evd_Model(
  (_deterministic_encoder): TransformerModel(
    (input_projection): Linear(in_features=4, out_features=64, bias=True)
    (transformer_encoder): TransformerEncoder(
      (layers): ModuleList(
        (0-1): 2 x TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=256, bias=True)
          (dropout): Dropout(p=0.3, inplace=False)
          (linear2): Linear(in_features=256, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.3, inplace=False)
          (dropout2): Dropout(p=0.3, inplace=False)
        )
      )
    )
    (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (fc): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Dropout(p=0.3, inplace=False)
      (3): Linear(in_features=64, out_features=64, bias=True)
    )
  )
  (_evidential_decoder): ANPEvidentialDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=67, out_features=64, bias=True)
    )
    (transform_gamma): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_v): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_alpha): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_beta): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
  )
)
Num parameters: 129988
Namespace(input_size=4, lstm_layers=2, lstm_hidden_size=64, lstm_dropout=0.3, channels=1, x_size=3, y_size=1, dataset='xauusd', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='CNP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=2.0, nig_nll_ker_reg_coef=0.5, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5)
Saved Details
Transformer_Evd_Model(
  (_deterministic_encoder): TransformerModel(
    (input_projection): Linear(in_features=4, out_features=64, bias=True)
    (transformer_encoder): TransformerEncoder(
      (layers): ModuleList(
        (0-1): 2 x TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=256, bias=True)
          (dropout): Dropout(p=0.3, inplace=False)
          (linear2): Linear(in_features=256, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.3, inplace=False)
          (dropout2): Dropout(p=0.3, inplace=False)
        )
      )
    )
    (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (fc): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Dropout(p=0.3, inplace=False)
      (3): Linear(in_features=64, out_features=64, bias=True)
    )
  )
  (_evidential_decoder): ANPEvidentialDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=67, out_features=64, bias=True)
    )
    (transform_gamma): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_v): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_alpha): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_beta): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
  )
)
Num parameters: 129988
Namespace(input_size=4, lstm_layers=2, lstm_hidden_size=64, lstm_dropout=0.3, channels=1, x_size=3, y_size=1, dataset='xauusd', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='CNP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=2.0, nig_nll_ker_reg_coef=0.5, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5)
Saved Details
Transformer_Evd_Model(
  (_deterministic_encoder): TransformerModel(
    (input_projection): Linear(in_features=4, out_features=64, bias=True)
    (transformer_encoder): TransformerEncoder(
      (layers): ModuleList(
        (0-1): 2 x TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=256, bias=True)
          (dropout): Dropout(p=0.3, inplace=False)
          (linear2): Linear(in_features=256, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.3, inplace=False)
          (dropout2): Dropout(p=0.3, inplace=False)
        )
      )
    )
    (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (fc): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Dropout(p=0.3, inplace=False)
      (3): Linear(in_features=64, out_features=64, bias=True)
    )
  )
  (_evidential_decoder): ANPEvidentialDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=67, out_features=64, bias=True)
    )
    (transform_gamma): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_v): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_alpha): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_beta): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
  )
)
Num parameters: 129988
Namespace(input_size=4, lstm_layers=2, lstm_hidden_size=64, lstm_dropout=0.3, channels=1, x_size=3, y_size=1, patience=1000, min_delta=1e-05, dataset='xauusd', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='CNP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=2.0, nig_nll_ker_reg_coef=0.5, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5)
Saved Details
Transformer_Evd_Model(
  (_deterministic_encoder): TransformerModel(
    (input_projection): Linear(in_features=4, out_features=64, bias=True)
    (transformer_encoder): TransformerEncoder(
      (layers): ModuleList(
        (0-1): 2 x TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=256, bias=True)
          (dropout): Dropout(p=0.3, inplace=False)
          (linear2): Linear(in_features=256, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.3, inplace=False)
          (dropout2): Dropout(p=0.3, inplace=False)
        )
      )
    )
    (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (fc): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Dropout(p=0.3, inplace=False)
      (3): Linear(in_features=64, out_features=64, bias=True)
    )
  )
  (_evidential_decoder): ANPEvidentialDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=67, out_features=64, bias=True)
    )
    (transform_gamma): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_v): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_alpha): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_beta): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
  )
)
Num parameters: 129988
Namespace(input_size=4, lstm_layers=2, lstm_hidden_size=64, lstm_dropout=0.3, channels=1, x_size=3, y_size=1, dataset='xauusd', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='CNP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=2.0, nig_nll_ker_reg_coef=0.5, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5)
Saved Details
Transformer_Evd_Model(
  (_deterministic_encoder): TransformerModel(
    (input_projection): Linear(in_features=4, out_features=64, bias=True)
    (transformer_encoder): TransformerEncoder(
      (layers): ModuleList(
        (0-1): 2 x TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=256, bias=True)
          (dropout): Dropout(p=0.3, inplace=False)
          (linear2): Linear(in_features=256, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.3, inplace=False)
          (dropout2): Dropout(p=0.3, inplace=False)
        )
      )
    )
    (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (fc): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Dropout(p=0.3, inplace=False)
      (3): Linear(in_features=64, out_features=64, bias=True)
    )
  )
  (_evidential_decoder): ANPEvidentialDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=67, out_features=64, bias=True)
    )
    (transform_gamma): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_v): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_alpha): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_beta): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
  )
)
Num parameters: 129988
Namespace(input_size=4, lstm_layers=2, lstm_hidden_size=64, lstm_dropout=0.3, channels=1, x_size=3, y_size=1, dataset='xauusd', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='CNP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=2.0, nig_nll_ker_reg_coef=0.5, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5)
Saved Details
Transformer_Evd_Model(
  (_deterministic_encoder): TransformerModel(
    (input_projection): Linear(in_features=4, out_features=64, bias=True)
    (transformer_encoder): TransformerEncoder(
      (layers): ModuleList(
        (0-1): 2 x TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=256, bias=True)
          (dropout): Dropout(p=0.3, inplace=False)
          (linear2): Linear(in_features=256, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.3, inplace=False)
          (dropout2): Dropout(p=0.3, inplace=False)
        )
      )
    )
    (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (fc): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Dropout(p=0.3, inplace=False)
      (3): Linear(in_features=64, out_features=64, bias=True)
    )
  )
  (_evidential_decoder): ANPEvidentialDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=67, out_features=64, bias=True)
    )
    (transform_gamma): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_v): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_alpha): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_beta): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
  )
)
Num parameters: 129988
Namespace(input_size=4, lstm_layers=2, lstm_hidden_size=64, lstm_dropout=0.3, channels=1, x_size=3, y_size=1, dataset='xauusd', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='CNP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=2.0, nig_nll_ker_reg_coef=0.5, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5)
Saved Details
Transformer_Evd_Model(
  (_deterministic_encoder): TransformerModel(
    (input_projection): Linear(in_features=4, out_features=64, bias=True)
    (transformer_encoder): TransformerEncoder(
      (layers): ModuleList(
        (0-1): 2 x TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=256, bias=True)
          (dropout): Dropout(p=0.3, inplace=False)
          (linear2): Linear(in_features=256, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.3, inplace=False)
          (dropout2): Dropout(p=0.3, inplace=False)
        )
      )
    )
    (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (fc): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Dropout(p=0.3, inplace=False)
      (3): Linear(in_features=64, out_features=64, bias=True)
    )
  )
  (_evidential_decoder): ANPEvidentialDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=67, out_features=64, bias=True)
    )
    (transform_gamma): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_v): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_alpha): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_beta): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
  )
)
Num parameters: 129988
Namespace(input_size=4, lstm_layers=2, lstm_hidden_size=64, lstm_dropout=0.3, channels=1, x_size=3, y_size=1, dataset='xauusd', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='CNP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=2.0, nig_nll_ker_reg_coef=0.5, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='False', outlier_val=0.0, active_task_sel=False, use_domain_knowledge=False, save_models_every=5)
Saved Details
Transformer_Evd_Model(
  (_deterministic_encoder): TransformerModel(
    (input_projection): Linear(in_features=4, out_features=64, bias=True)
    (transformer_encoder): TransformerEncoder(
      (layers): ModuleList(
        (0-1): 2 x TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=256, bias=True)
          (dropout): Dropout(p=0.3, inplace=False)
          (linear2): Linear(in_features=256, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.3, inplace=False)
          (dropout2): Dropout(p=0.3, inplace=False)
        )
      )
    )
    (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (fc): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Dropout(p=0.3, inplace=False)
      (3): Linear(in_features=64, out_features=64, bias=True)
    )
  )
  (_evidential_decoder): ANPEvidentialDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=67, out_features=64, bias=True)
    )
    (transform_gamma): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_v): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_alpha): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_beta): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
  )
)
Num parameters: 129988
Namespace(input_size=4, lstm_layers=2, lstm_hidden_size=64, lstm_dropout=0.3, channels=1, x_size=3, y_size=1, dataset='xauusd', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='CNP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=2.0, nig_nll_ker_reg_coef=0.5, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='True', outlier_val=1, active_task_sel=False, use_domain_knowledge=False, save_models_every=5)
Saved Details
Transformer_Evd_Model(
  (_deterministic_encoder): TransformerModel(
    (input_projection): Linear(in_features=4, out_features=64, bias=True)
    (transformer_encoder): TransformerEncoder(
      (layers): ModuleList(
        (0-1): 2 x TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=256, bias=True)
          (dropout): Dropout(p=0.3, inplace=False)
          (linear2): Linear(in_features=256, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.3, inplace=False)
          (dropout2): Dropout(p=0.3, inplace=False)
        )
      )
    )
    (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (fc): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Dropout(p=0.3, inplace=False)
      (3): Linear(in_features=64, out_features=64, bias=True)
    )
  )
  (_evidential_decoder): ANPEvidentialDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=67, out_features=64, bias=True)
    )
    (transform_gamma): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_v): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_alpha): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_beta): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
  )
)
Num parameters: 129988
Namespace(input_size=4, lstm_layers=2, lstm_hidden_size=64, lstm_dropout=0.3, channels=1, x_size=3, y_size=1, dataset='xauusd', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='CNP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=2.0, nig_nll_ker_reg_coef=0.5, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='True', outlier_val=1, active_task_sel=False, use_domain_knowledge=False, save_models_every=5)
Saved Details
Transformer_Evd_Model(
  (_deterministic_encoder): TransformerModel(
    (input_projection): Linear(in_features=5, out_features=64, bias=True)
    (transformer_encoder): TransformerEncoder(
      (layers): ModuleList(
        (0-1): 2 x TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=256, bias=True)
          (dropout): Dropout(p=0.3, inplace=False)
          (linear2): Linear(in_features=256, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.3, inplace=False)
          (dropout2): Dropout(p=0.3, inplace=False)
        )
      )
    )
    (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (fc): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Dropout(p=0.3, inplace=False)
      (3): Linear(in_features=64, out_features=64, bias=True)
    )
  )
  (_evidential_decoder): ANPEvidentialDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=68, out_features=64, bias=True)
    )
    (transform_gamma): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_v): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_alpha): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_beta): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
  )
)
Num parameters: 130116
Namespace(input_size=5, lstm_layers=2, lstm_hidden_size=64, lstm_dropout=0.3, channels=1, x_size=4, y_size=1, dataset='xauusd', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='CNP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=2.0, nig_nll_ker_reg_coef=0.5, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='True', outlier_val=1, active_task_sel=False, use_domain_knowledge=False, save_models_every=5)
Saved Details
Transformer_Evd_Model(
  (_deterministic_encoder): TransformerModel(
    (input_projection): Linear(in_features=5, out_features=64, bias=True)
    (transformer_encoder): TransformerEncoder(
      (layers): ModuleList(
        (0-1): 2 x TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=256, bias=True)
          (dropout): Dropout(p=0.3, inplace=False)
          (linear2): Linear(in_features=256, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.3, inplace=False)
          (dropout2): Dropout(p=0.3, inplace=False)
        )
      )
    )
    (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (fc): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Dropout(p=0.3, inplace=False)
      (3): Linear(in_features=64, out_features=64, bias=True)
    )
  )
  (_evidential_decoder): ANPEvidentialDecoder(
    (linear_layers_list): ModuleList(
      (0): Linear(in_features=68, out_features=64, bias=True)
    )
    (transform_gamma): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_v): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_alpha): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
    (transform_beta): Sequential(
      (0): ReLU()
      (1): Linear(in_features=64, out_features=64, bias=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=1, bias=True)
    )
  )
)
Num parameters: 130116
Namespace(input_size=5, lstm_layers=2, lstm_hidden_size=64, lstm_dropout=0.3, channels=1, x_size=4, y_size=1, dataset='xauusd', seed=0, training_iterations=20000, num_epochs=50, test_1d_every=2000, save_results_every=1, num_test_tasks=2000, max_context_points=50, model_type='CNP', attention_type='multihead', gpu_id=0, epochs=50, batch_size=8, experiment_name='CNP-model-save-name', representation_size=128, hidden_size=128, num_enc_hdn_lrs=4, learning_rate=0.001, use_deterministic_path=True, use_latent_path=False, debugging=False, load_model=False, nig_nll_reg_coef=2.0, nig_nll_ker_reg_coef=0.5, ev_dec_beta_min=0.2, ev_dec_alpha_max=20.0, ev_dec_v_max=20.0, ev_lat_beta_min=0.2, ev_lat_alpha_max=20.0, ev_lat_v_max=20.0, outlier_training_tasks='True', outlier_val=1, active_task_sel=False, use_domain_knowledge=False, save_models_every=5)
